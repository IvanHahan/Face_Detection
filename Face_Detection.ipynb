{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from matplotlib import image\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.merge import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import h5py as h5py\n",
    "# import imgaug as ia\n",
    "# from tqdm import tqdm\n",
    "# from imgaug import augmenters as iaa\n",
    "# import numpy as np\n",
    "import pickle\n",
    "import os, cv2\n",
    "# from preprocessing import parse_annotation, BatchGenerator\n",
    "# from utils import WeightReader, decode_netout, draw_boxes, normalize\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_urls = np.loadtxt('faces.csv', dtype=np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_images_from_urls(urls):\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(urls[i], 'images/img' + str(i) +'.png' )\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Basic-Yolo-Keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "IMAGE_H, IMAGE_W = 416, 416\n",
    "GRID_H,  GRID_W  = 13 , 13\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD    = 0.3#0.5\n",
    "NMS_THRESHOLD    = 0.3#0.45\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 16\n",
    "WARM_UP_BATCHES  = 0\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def space_to_depth_x2(x):\n",
    "    return tf.space_to_depth(x, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "# Layer 1\n",
    "x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
    "x = BatchNormalization(name='norm_1')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 2\n",
    "x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_2')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 3\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_3')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 4\n",
    "x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_4')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 5\n",
    "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_5')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 6\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_6')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 7\n",
    "x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_7')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 8\n",
    "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_8')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 9\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_9')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 10\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_10')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 11\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_11')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 12\n",
    "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_12')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 13\n",
    "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_13')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "skip_connection = x\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Layer 14\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_14')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 15\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_15')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 16\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_16')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 17\n",
    "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_17')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 18\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_18')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 19\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_19')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 20\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_20')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 21\n",
    "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
    "\n",
    "x = concatenate([skip_connection, x])\n",
    "\n",
    "# Layer 22\n",
    "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "x = BatchNormalization(name='norm_22')(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "# Layer 23\n",
    "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "\n",
    "# small hack to allow true_boxes to be registered when Keras build the model \n",
    "# for more information: https://github.com/fchollet/keras/issues/2790\n",
    "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "model = Model([input_image, true_boxes], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 44 layers into a model with 45 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d31b0cdb448f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'full_yolo_backend.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m                 load_weights_from_hdf5_group(\n\u001b[0;32m-> 2652\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   2653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   3159\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   3162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 44 layers into a model with 45 layers."
     ]
    }
   ],
   "source": [
    "model.load_weights('full_yolo_backend.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
